{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тема № 03. Работа с текстовыми данными\n",
    "\n",
    "*Текстовый* файл, пожалуй, самый простой тип файла для человека, хранит текст - последовательность символов. Каждый символ представляется некоторым числом, и в компьютерах есть *таблицы символов*, которые показывают какое число соответствует символу (есть несколько разных таблицы, мы сегодня будем работать только с одной из них). Эти числа одинаковой длины, т.е. в них одно и то же количество бит (обычно один или два байта). Программы, которые работают с текстовыми файлами, конечно, обрабатывают последовательности чисел по специальным правилам, но отображают эти числа в виде символов, так и получается видимый нами текст. \n",
    "\n",
    "Чтобы работать с файлами, текстовыми или любыми другими, сначала нужно как-то сказать компьютеру, с каким именно файлом мы будем работать. Говорят, что мы должны __открыть__ файл.\n",
    "__Открыть__ файл значит разрешить к нему доступ, когда файл больше не нужен, его нужно __закрыть__, т.е. прекратить доступ к нему. Чтобы понимать к какому именно файлу мы получаем доступ, по каким правилам разрешаем его обрабатывать команда открытия создаст некоторый *идентификатор*, ссылку на файл, который мы можем назвать *поток*. Операционная система будет строго следить за тем, чтобы работа с файлом не нарушала правил, если мы при открытии разрешили только читать файл, то не сможем в него ничего записать. Иногда операционная система может помешать нам в некоторых действиях с файлами, например удалить его, если нет прав на удаление и т.п. Тут нужно быть внимательным и всегда поверять открылся ли нужный файл или нет. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  1. Открытие, закрытие и чтение текстовых файлов\n",
    "Текстовые файлы открываются встроенной функцией `open()`. Мы указываем путь имя и расширение файла, правила как его открыть, в том числе указываем открываем файл в текстовом виде (по умолчанию) или в бинарном.  \n",
    "\n",
    "Команда возвращает файловый объект, у которого есть метод `read()` для чтения содержимого файла. Если открыть файл не получилось (например, нет такого файла), то вернется ошибка. \n",
    "\n",
    "Давайте откроем для чтения и прочитаем файл eng_text.txt, он расположен в той же папке, в которой мы работаем, поэтому он будет легко найден.  \n",
    "\n",
    "Попробуйте поменять имя файла на несуществующее и посмотреть, что получится.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-11T07:35:00.863451Z",
     "start_time": "2020-10-11T07:35:00.856450Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! Welcome to demofile.txt\n",
      "This file is for testing purposes.\n",
      "Good Luck!\n"
     ]
    }
   ],
   "source": [
    "f = open(\"eng_text.txt\", \"r\")\n",
    "print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По умолчанию `read()` возвращает все содержимое файла, но также можно задать количество символов, которые будут возвращены после прочтения.\n",
    "\n",
    "Файл читается последовательно, если мы прочитали 6 символов из него, и опять выполним команду для чтения 6 символов, то нам вернутся следующие 6 символов, а не те же самые. До тех пор, пока не достигнем конца файла, потом команда чтения будет возвращать пустую переменную.\n",
    "\n",
    "Кстати, пробелы, запятые, точки - это тоже символы и они также читаются.\n",
    "\n",
    "Попробуйте прочитать больше символов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-11T07:35:00.921454Z",
     "start_time": "2020-10-11T07:35:00.917454Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello!\n",
      " Welc\n"
     ]
    }
   ],
   "source": [
    "f = open(\"eng_text.txt\", \"r\")\n",
    "print(f.read(6))\n",
    "print(f.read(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод `readline()` позволяет читать текстовый файл по строкам. А в файлах, чтобы разделить строки между собой, вставляются специальные символы, они не видны на экране, но есть в файле.\n",
    "\n",
    "(Мы каждый раз открываем файл по-новому, чтобы читать его сначала, а не продолжать с того места где остановились)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-11T07:35:00.981458Z",
     "start_time": "2020-10-11T07:35:00.977457Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! Welcome to demofile.txt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f = open(\"eng_text.txt\", \"r\")\n",
    "print(f.readline())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-11T07:35:01.014459Z",
     "start_time": "2020-10-11T07:35:01.011459Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This file is for testing purposes.\n",
      "\n",
      "Good Luck!\n"
     ]
    }
   ],
   "source": [
    "print(f.readline())\n",
    "print(f.readline())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-11T07:35:01.047461Z",
     "start_time": "2020-10-11T07:35:01.041461Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "joblib.dump(data, 'data/data.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кстати, в этой функции тоже можно указать дополнительный аргумент, равный количеству символов для прочтения. Обратите внимание как именно при этом будут прочитаны строки. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-11T07:35:01.102465Z",
     "start_time": "2020-10-11T07:35:01.097464Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! Welcome to \n",
      "demofile.txt\n",
      "\n",
      "This file is for t\n",
      "esting purposes.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f = open(\"eng_text.txt\", \"r\")\n",
    "\n",
    "print(f.readline(18))\n",
    "print(f.readline(18))\n",
    "print(f.readline(18))\n",
    "print(f.readline(18))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод `readlines()` позволяет прочитать все строки из файла в список, разбив по строкам.\n",
    "\n",
    "Ниже мы получим список из трех элементов, каждый будет содержат свою строку.\n",
    "\n",
    "Обратите внимание на `\\n` это обозначение специального символа переноса строки, который есть в файле. Это только обозначение, в файле такой символ хранится как число."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-11T07:35:01.156468Z",
     "start_time": "2020-10-11T07:35:01.153467Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello! Welcome to demofile.txt\\n', 'This file is for testing purposes.\\n', 'Good Luck!']\n"
     ]
    }
   ],
   "source": [
    "f = open(\"eng_text.txt\", \"r\")\n",
    "print(f.readlines())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Похожий результат можно получить, читая файл в цикле. Тут мы явно не вызываем команду для чтения, но она вызывается автоматически при итерировании"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-11T07:35:01.211471Z",
     "start_time": "2020-10-11T07:35:01.208471Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! Welcome to demofile.txt\n",
      "\n",
      "This file is for testing purposes.\n",
      "\n",
      "Good Luck!\n"
     ]
    }
   ],
   "source": [
    "f = open(\"eng_text.txt\", \"r\")\n",
    "for x in f:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Встроенная функция `close()` закрывает файл. При закрытии файла все ресурсы освобождаются, и больше не занимают память. Важно закрывать файлы всегда, когда они больше не нужны. Иначе в системе останется мусор, который будет занимать лишнюю память и замедлять работу. Это не страшно, когда файлов мало, но когда одновременно открыто много файлов, это может существенно повлиять на работу компьютера.\n",
    "\n",
    "Выше мы не закрывали файлы, хотя это нужно было делать.\n",
    "\n",
    "После закрытия файла мы уже не можем с ним работать. Попробуйте что-то прочитать после закрытия."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-11T07:35:01.270474Z",
     "start_time": "2020-10-11T07:35:01.266474Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! Welcome to demofile.txt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f = open(\"eng_text.txt\", \"r\")\n",
    "print(f.readline())\n",
    "f.close()\n",
    "# print(f.readline())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Форматы и режимы работы с файлами"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Основные форматы файлов это *текстовый* и *бинарный*. По сути, они одинаковы, и текстовый и бинарный файл это последовательности байт, но интерпретируются они по-разному. Мы научились читать текстовые файлы. Рассмотренные функции работают также и для чтения бинарных файлов.\n",
    "\n",
    "Для работы с другими форматами файлов их тоже придется открывать, закрывать, но работать с ними будем с помощью других функций. Некоторые из них мы рассмотрим на последующих уроках.\n",
    "\n",
    "Файлы можно не только читать - получать информацию из них, но и писать - записывать информацию в файл.\n",
    "\n",
    "Что именно мы хотим сделать с файлом можно указать в дополнительном аргументе `mode` команды `open()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`'r'` : Открыть файл только для чтения (Read), запись в него запрещена. \n",
    "\n",
    "`'w'` : Открыть файл для записи (Write). Если файл уже существует все данные в нем будут стерты. Если файла нет, он будет создан. Читать такой файл нельзя.\n",
    "\n",
    "`'a'` : Открыть файл для записи с добавлением. Если файла нет, он будет создан. Если файл уже существует все данные в нем сохранятся, а новые данные будут добавляться после старых.\n",
    "\n",
    "`'+'` : Открыть файл для модификаций и на чтение и на запись. При этом указывая `'r+'` мы сохраняем старое содержание файла, `'w+'` - очищаем файл. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Запись в текстовый файл"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Есть две встроенные функции, чтобы записать текст (строки) в файл:\n",
    "\n",
    "`write()` : Записывает одну строку `str1` в текстовый файл.\n",
    "\n",
    "`writelines()` : Записывает в текстовый файл *список* строк, разделители строк не добавляются, если это нужно надо создавать самим.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-11T07:35:01.426483Z",
     "start_time": "2020-10-11T07:35:01.421483Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df['target'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-11T07:35:01.482486Z",
     "start_time": "2020-10-11T07:35:01.452485Z"
    }
   },
   "outputs": [],
   "source": [
    "output_file.write(\"Привет \\n\") # запишем в файл строку \"Привет\" с переносом\n",
    "output_file.writelines(L) # запишем созданный ранее список строк\n",
    "output_file.close() # закроем файл"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь откроем файл для модификаций, и прочитаем что в нем записано. Можете также посмотреть на диске. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-11T07:35:01.546490Z",
     "start_time": "2020-10-11T07:35:01.509488Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "В том файле написано: \n",
      "\n",
      "Привет \n",
      "Строка 2: Мы \n",
      "Строка 3: хотим \n",
      "Строка 4: что-то записать \n",
      "\n"
     ]
    }
   ],
   "source": [
    "output_file = open(\"myfile.txt\",\"r+\")  # откроем файл для модификаций, r+ поэтому содержимое сохранится.\n",
    "\n",
    "print(\"В том файле написано: \\n\") #\n",
    "print(output_file.read()) # прочитаем содержимое и выведем на экран"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-11T07:35:01.576492Z",
     "start_time": "2020-10-11T07:35:01.548490Z"
    }
   },
   "outputs": [],
   "source": [
    "output_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разберем подробнее, что мы только что сделали:\n",
    "\n",
    "1) мы создали файловый объект - открыли файл. Обратите внимание на режим - `'w'` - запись.\n",
    "\n",
    "2) Создали список строк `L` в котором содержится три строки с произвольным текстом, которые мы хотели бы записать в файл\n",
    "\n",
    "3) записали в файл строку \"Привет\"\n",
    "\n",
    "4) записали в файл ранее сформированный список строк `L`\n",
    "\n",
    "5) закрыли файл\n",
    "\n",
    "6) открыли файл в режиме модификаций `r+`\n",
    "\n",
    "7) прочитали что в нем записано.\n",
    "\n",
    "8) закрыли файл."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание, что можно записывать либо как в пункте 3, либо как в пункте 4 - не обязательно использовать оба сразу. Надо выбирать, в какой ситуации удобней записывать по одной строке за раз, а в какой все строки в списке."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Добавление текста к файлу"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте разберем ситуацию, когда файл уже существует, в нем записана какая-то информация, и надо, не уничтожая ее, записать в файл еще что-то. Такое встречается постоянно, как пример - запись файлов логов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Откроем файл с соответствующим режимом `'a'` (append) и запишем туда какую-нибудь новую информацию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-11T07:35:01.689498Z",
     "start_time": "2020-10-11T07:35:01.685498Z"
    }
   },
   "outputs": [],
   "source": [
    "appended_file = open(\"myfile.txt\",\"a\") # открыли файл для добавления\n",
    "appended_file.write(\"Мы открыли файл и добавили туда эту строку \\n\") # добавили в него текст\n",
    "appended_file.close() # закрыли файл"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте теперь посмотрим, что хранится в этом файле:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-11T07:35:01.752502Z",
     "start_time": "2020-10-11T07:35:01.747501Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "После добавления в файле оказалось: \n",
      "\n",
      "['Привет \\n', 'Строка 2: Мы \\n', 'Строка 3: хотим \\n', 'Строка 4: что-то записать \\n', 'Мы открыли файл и добавили туда эту строку \\n']\n"
     ]
    }
   ],
   "source": [
    "sns.countplot(train_df['target'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Новая строка добавлена, старые сохранились."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Перезапись файла"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Бывают ситуации, когда информацию файла надо не сохранить, а полностью перезаписать (с другой стороны, если ее перезаписывать не надо, то надо точно знать, что так делать нельзя).\n",
    "\n",
    "Давайте попробуем перезаписать файл. Для этого можно использовать все те же конструкции, просто открыть файл не для добавления, а для записи (с уже знакомым режимом `'w'`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-11T07:35:01.915511Z",
     "start_time": "2020-10-11T07:35:01.845507Z"
    }
   },
   "outputs": [],
   "source": [
    "rewrited_file = open(\"myfile.txt\",\"w\") # откроем файл для записи\n",
    "rewrited_file.write(\"Мы перезаписали этот файл \\n\")  # запишем\n",
    "rewrited_file.close() # закроем"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим, что получилось:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-11T07:35:01.949513Z",
     "start_time": "2020-10-11T07:35:01.917511Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "После перезаписи в файле осталось только: \n",
      "\n",
      "['Мы перезаписали этот файл \\n']\n"
     ]
    }
   ],
   "source": [
    "rewrited_file_check = open(\"myfile.txt\",\"r\")  # откроем для чтения\n",
    "print(\"После перезаписи в файле осталось только: \\n\") #\n",
    "print(rewrited_file_check.readlines())  # прочитаем\n",
    "rewrited_file_check.close() # закроем"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Работа со строками"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Когда мы прочитали текст из файла, получили наборы строк в памяти. Их можно обрабатывать, выполнять разные действия со строками."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Строка - это массив символов, со строками можно обращаться как с массивами и делать доступ по индексам. То есть, какое-то слово можно получить, зная его местоположение в строке и количество символов в нем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-11T07:35:02.024517Z",
     "start_time": "2020-10-11T07:35:02.016517Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'строка'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 'Эта строка' # сделаем строку\n",
    "s[4:] # получим элементы, начиная с пятого (помним что индексы начинаются с нуля!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Узнать количество символов в строке, ее длину можно командой `len()`. Она считает все элементы, включая пробелы, непечатаемые символы и др. Вставьте, например, символ `\\n` в конец строки, хоть он написан нами двумя символами ' \\ ' и ' n ' это управляющий символ и считается за один."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-11T07:35:02.081521Z",
     "start_time": "2020-10-11T07:35:02.078520Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(s)# длина строки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-11T07:35:02.121523Z",
     "start_time": "2020-10-11T07:35:02.111522Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Эта строка\n",
      "10\n",
      "Эта строка\n",
      "\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "train_time = train_df[times]\n",
    "train_time.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наоборот, по нужному слову (строке) найти его местоположение в нашей строке можно при помощи функции `find()`. Она вернет нам индекс элемента, с которого слово начинается, а если такое слово не найдено, то вернет -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-11T07:35:02.176526Z",
     "start_time": "2020-10-11T07:35:02.172526Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 'Эта строка' # сделаем строку\n",
    "s.find('строка')# попробуйте поискать другое слово"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-11T07:35:02.220528Z",
     "start_time": "2020-10-11T07:35:02.202527Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'строка'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s[s.find('строка'):] # проверим что найден именно нужный индекс, выведем строку начиная с этого найденного индекса. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А как по местоположению начала слова получить все слово без остального текста?\n",
    "\n",
    "Зная индекс и длину - просто."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-11T07:35:02.266531Z",
     "start_time": "2020-10-11T07:35:02.262531Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'строка'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s[s.find('строка'):s.find('строка') + len('строка')] # вот так"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-11T07:35:02.309534Z",
     "start_time": "2020-10-11T07:35:02.294533Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.find('строка') # первый find вернул нам индекс 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-11T07:35:02.345536Z",
     "start_time": "2020-10-11T07:35:02.327535Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session_start_hour = train_time['time1'].apply(lambda ts: ts.hour).values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы разбить строки (тексты) на более простые части (предложения, слова, или что-то более специфичное), можно использовать функцию `split()`. Мы указываем строку-разделитель, по которой будет проходить разбиение, она может быть любой, хоть цифра, хоть буква, хоть знак препинания и даже несколько символов. Сама строка-разделитель пропадет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-11T07:35:02.388538Z",
     "start_time": "2020-10-11T07:35:02.381538Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Эта', 'строка']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.split(' ') # символ-разделитель - пробел. Получим две строки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-11T07:35:02.427540Z",
     "start_time": "2020-10-11T07:35:02.411539Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Большой и интересный текст это',\n",
       " ' Привет, мир, - говорит программа',\n",
       " ' Что-то оптимизировалось',\n",
       " '']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 'Большой и интересный текст это. Привет, мир, - говорит программа. Что-то оптимизировалось.'\n",
    "l = s.split('.') # символ-разделитель - точка .\n",
    "s.split('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы обратно склеить разбитые части, можно использовать функцию `join()`. При этом в аргумент мы подаем список строк, которые надо склеить в одну, а сам метод применяем к строке, которую будем использовать как клей (она была строкой-разделителем в предыдущей команде)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-11T07:35:02.469543Z",
     "start_time": "2020-10-11T07:35:02.465542Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Большой и интересный текст это  Привет, мир, - говорит программа  Что-то оптимизировалось '"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(l) # строка- клей - пробел ' '"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание, что получившийся текст отличается от первоначального. Когда мы разбивали на предложения с символом-разделителем '.', то они исчезли, а когда мы склеивали обратно через пробел, точки ниоткуда не взялись. Исправим это и сравним с исходным текстом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-11T07:35:02.523546Z",
     "start_time": "2020-10-11T07:35:02.519546Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Большой и интересный текст это. Привет, мир, - говорит программа. Что-то оптимизировалось.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'.'.join(l) # строка- клей - точка '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-11T07:35:02.555548Z",
     "start_time": "2020-10-11T07:35:02.550547Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sns.countplot(session_start_hour)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Обработка текста модулем `spacy`\n",
    "\n",
    "Простые функции обработки строк ничего не знают о тексте, о правилах языка. Но часто нужна более продвинутая обработка, например узнать все имена существительные в тексте. Для этого нужно использовать специальные библиотеки. Сегодня мы кратко рассмотрим две из них: `spacy` и `nltk`. Когда же будем более подробно заниматься текстом, мы эти библиотеки рассмотрим более подробно. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Библиотека __spacy__  (описание см. https://nlpub.mipt.ru/SpaCy  https://spacy.io/usage/spacy-101) \n",
    "предназначена для обработки текстов на естественном языке (NLP, Natural Language Processing). \n",
    "\n",
    "Если она еще не установлена на компьютере - надо установить ее, дополнительную библиотеку __textacy__ (https://github.com/chartbeat-labs/textacy), загрузить модель (для английского языка, ~1Гб):\n",
    "```\n",
    "# Установка spaCy \n",
    "pip3 install -U spacy\n",
    "\n",
    "# Загрузка модели для анализа английского языка\n",
    "python3 -m spacy download en_core_web_lg\n",
    "\n",
    "# Установка textacy\n",
    "pip3 install -U textacy\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сначала мы подключим библиотеку ```import spacy```\n",
    "\n",
    "Загрузим в память модель для английского языка ``` nlp = spacy.load('en_core_web_lg') ``` \n",
    "(для русского пока нет)\n",
    "\n",
    "en_core_web_lg это название модели, которая была заранее скачена на диск.\n",
    "\n",
    "Создадим некоторый текст `text`, который будем обрабатывать, можете создать свой.\n",
    "\n",
    "Выполним __парсинг__ текста командой `nlp()`. Парсинг проанализирует текст и определит типы слов\\словосочетаний, к чему они относятся.\n",
    "Например, можем узнать все __именованные сущности__ `ents`: найденные имена собственные, слова (словосочетания) которые указывают на положение объектов, слова которые описывают дату\\время и др. Их много разных типов: https://spacy.io/api/annotation#named-entities\n",
    "Обозначение и смысл их приведен ниже:\n",
    "```\n",
    "PERSON\tЛюди (имена, фамилии, прозвища и т.п.), в том числе вымышленные.\n",
    "NORP\tНазвания национальных, религиозных, политических объединений.\n",
    "FAC\tНазвания зданий, аэропортов, шоссе, мостов ... \n",
    "ORG\tНазвания компаний, агентств, организаций ...\n",
    "GPE\tНазвания стран, городов, штатов (округов) ...\n",
    "LOC\tНазвания географических областей (кроме относящихся к GPE), горных массивов, водоемов... \n",
    "PRODUCT\tНазвания товаров, автомобилей, еды (кроме услуг) ...\n",
    "EVENT\tНазвания событий, ураганов, битв, войн, спортивных состязаний...\n",
    "WORK_OF_ART\tНазвания произведений искусств, книг, картин, песен... \n",
    "LAW\tНазвания юридических документов\n",
    "LANGUAGE\tНазвание языка\n",
    "DATE\tУказание на даты и периоды, абсолютные или относительные\n",
    "TIME\tУказание на время (меньше дня)\n",
    "PERCENT\tУказания на проценты ”%“\n",
    "MONEY\tУказания на деньги, значение и единицы\n",
    "QUANTITY\tУказания на количество чего-либо, вес, размер, .... \n",
    "ORDINAL\tУказание на порядок “первый”, “второй”, и так далее\n",
    "CARDINAL\tЧисла, которые не попали в другие категории\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-11T07:35:02.720557Z",
     "start_time": "2020-10-11T07:35:02.663554Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-572e663734a0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mspacy\u001b[0m \u001b[1;31m# подключим библиотеку\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Загрузим NLP-модель для английского языка\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mnlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'en_core_web_lg'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# en_core_web_lg это название модели, которая была скачена и установлена\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "import spacy # подключим библиотеку\n",
    "\n",
    "# Загрузим NLP-модель для английского языка\n",
    "nlp = spacy.load('en_core_web_lg') # en_core_web_lg это название модели, которая была скачена и установлена\n",
    "\n",
    "# Текст для анализа. Можете написать свой текст (на английском)\n",
    "text = \"\"\"London is the capital and most populous city of England and \n",
    "the United Kingdom.  Standing on the River Thames in the south east \n",
    "of the island of Great Britain, London has been a major settlement \n",
    "for two millennia. It was founded by the Romans, who named it Londinium.\n",
    "\"\"\"\n",
    "\n",
    "# Парсинг текста с помощью spaCy. Эта команда запускает целый конвейер по обработке текста\n",
    "doc = nlp(text)\n",
    "\n",
    "# в переменной 'doc' теперь содержится обработанная версия текста\n",
    "# мы можем делать с ней все что угодно!\n",
    "# например, распечатать все обнаруженные именованные сущности (в .ents)\n",
    "for entity in doc.ents:\n",
    "    print(f\"{entity.text} ({entity.label_})\")# печатаем слово .text (словосочетание) и его тип .label_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание, модели могут и ошибаться. Почему Londinium отнесен к организациям а не городам? Так получилось... \n",
    "\n",
    "Попробуйте обработать свой текст. Для этого или исправьте текст выше или загрузите свой - вы уже умеете это делать."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Обработка текста модулем `nltk`\n",
    "Библиотека `nltk` также предназначена для работы с текстом на естественном языке.\n",
    "Документацию смотрите на https://www.nltk.org/\n",
    "Библиотеку необходимо установить заранее.\n",
    "\n",
    "Сама библиотека лишь предоставляет функции для обработки, правила обработки, какой вид обработки делать, зависит от *модели* которая будет использоваться.\n",
    "Один из видов обработки это __токенизация__.\n",
    "\n",
    "\n",
    "Токенизация (иногда – сегментация) - это разбиение текста на части, по словам, по предложениям и т.п.\n",
    "\n",
    "Для английского языка для токенизации мы загрузим модель `'punkt'` с помощью функции `nltk.download()`. Модель уже была обучена на большом количестве текстов, чтобы правильно проводить токенизацию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-11T07:35:05.619723Z",
     "start_time": "2020-10-11T07:35:02.818563Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\neuron\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "#import nltk.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Токенизация по предложениям\n",
    "\n",
    "Токенизация по предложениям – это процесс разделения письменного языка на предложения-компоненты. В английском и некоторых других языках мы можем вычленять предложение каждый раз, когда находим определенный знак пунктуации – точку.\n",
    "\n",
    "Но даже в английском эта задача нетривиальна, так как точка используется и в сокращениях. Таблица сокращений может сильно помочь во время обработки текста, чтобы избежать неверной расстановки границ предложений. В большинстве случаев для этого используются библиотеки, так что можете особо не переживать о деталях реализации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы сделать токенизацию предложений с помощью NLTK, можно воспользоваться методом `nltk.sent_tokenize`\n",
    "\n",
    "Конечно модели могут и ошибаться, попробуйте сделать текст, чтобы обмануть токенайзер."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-11T07:35:05.675726Z",
     "start_time": "2020-10-11T07:35:05.622723Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We try to implement NLTK.Sent_tokenize.\n",
      "\n",
      "It is very hard to produce good tokens.\n",
      "\n",
      "Our approach is model-based one!\n",
      "\n",
      "And they has already train a good model for tokenizing.\n",
      "\n",
      "Really?\n",
      "\n",
      "Yes ... try.\n",
      "\n",
      "Hard words: vice president, half sister\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"We try to implement NLTK.Sent_tokenize. It is very hard to produce good tokens. Our approach is model-based one! And they has already train a good model for tokenizing. Really? Yes ... try. Hard words: vice president, half sister\"\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "for sentence in sentences:\n",
    "    print(sentence)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Токенизация по словам\n",
    "\n",
    "Токенизация (иногда – сегментация) по словам – это процесс разделения предложений на слова-компоненты. В английском и многих других языках, использующих ту или иную версию латинского алфавита, пробел – это неплохой разделитель слов.\n",
    "\n",
    "Тем не менее, могут возникнуть проблемы, если мы будем использовать только пробел – в английском составные существительные пишутся по-разному и иногда через пробел. И тут вновь нам помогают библиотеки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмем предложения из предыдущего блока кода и применим к каждому из них метод `word_tokenize()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-11T07:35:05.682726Z",
     "start_time": "2020-10-11T07:35:05.677726Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['We', 'try', 'to', 'implement', 'NLTK.Sent_tokenize', '.']\n",
      "\n",
      "['It', 'is', 'very', 'hard', 'to', 'produce', 'good', 'tokens', '.']\n",
      "\n",
      "['Our', 'approach', 'is', 'model-based', 'one', '!']\n",
      "\n",
      "['And', 'they', 'has', 'already', 'train', 'a', 'good', 'model', 'for', 'tokenizing', '.']\n",
      "\n",
      "['Really', '?']\n",
      "\n",
      "['Yes', '...', 'try', '.']\n",
      "\n",
      "['Hard', 'words', ':', 'vice', 'president', ',', 'half', 'sister']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sentence in sentences:\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    print(words)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Лемматизация и стемминг текста"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обычно тексты содержат разные грамматические формы одного и того же слова, а также могут встречаться однокоренные слова. Лемматизация и стемминг преследуют цель привести все встречающиеся словоформы к одной, нормальной словарной форме."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лемматизация и стемминг – это частные случаи нормализации и они отличаются.\n",
    "\n",
    "Стемминг – это грубый эвристический процесс, который отрезает «лишнее» от корня слов, часто это приводит к потере словообразовательных суффиксов.\n",
    "\n",
    "Лемматизация – это более тонкий процесс, который использует словарь и морфологический анализ, чтобы в итоге привести слово к его канонической форме – лемме.\n",
    "\n",
    "Отличие в том, что стеммер (конкретная реализация алгоритма стемминга) действует без знания контекста и, соответственно, не понимает разницу между словами, которые имеют разный смысл в зависимости от части речи. Однако у стеммеров есть и свои преимущества: их проще внедрить и они работают быстрее. Плюс, более низкая «аккуратность» может не иметь значения в некоторых случаях."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Примеры:\n",
    "\n",
    "Слово good – это лемма для слова better. Стеммер не увидит эту связь, так как здесь нужно сверяться со словарем.\n",
    "Слово play – это базовая форма слова playing. Тут справятся и стемминг, и лемматизация.\n",
    "Слово meeting может быть как нормальной формой существительного, так и формой глагола to meet, в зависимости от контекста. В отличие от стемминга, лемматизация попробует выбрать правильную лемму, опираясь на контекст.\n",
    "\n",
    "Теперь, когда мы знаем, в чем разница, давайте рассмотрим пример.\n",
    "\n",
    "Сначала мы подключим модель стемминга  `PorterStemmer` и лемматизации `WordNetLemmatizer`, загрузим и подключим \"корпус\" `wordnet`  в котором много слов, синонимов и т.п.\n",
    "\n",
    "Создадим функцию `compare_stemmer_and_lemmatizer()`, которая будет приводить слова к нормальной форме. Ей мы укажем каким стеммером и леммером (есть такое слово, а?) пользоваться, само слово, которое надо лемматизировать, и часть речи `POS` желаемого результата (`VERB`  - глагол, `NOUN` - существительное).\n",
    "```\n",
    "Обозначение \t Значение \t    Примеры\n",
    "ADJ              прилагательное  new, good, high, special, big, local\n",
    "ADP              предлог         on, of, at, with, by, into, under\n",
    "ADV              наречие         really, already, still, early, now\n",
    "CONJ             союз            and, or, but, if, while, although\n",
    "DET              артикль,        \n",
    "                 определитель    the, a, some, most, every, no, which\n",
    "NOUN             существительное year, home, costs, time, Africa\n",
    "NUM              числительное    twenty-four, fourth, 1991, 14:24\n",
    "PRT              частица         at, on, out, over per, that, up, with\n",
    "PRON             местоимение     he, their, her, its, my, I, us\n",
    "VERB             глагол          is, say, told, given, playing, would\n",
    ".                знак пунктуации . , ; !\n",
    "X                другое          ersatz, esprit, dunno, gr8, univeristy\n",
    "\n",
    "```\n",
    "Стеммер вызывается функцией `stem(word)`, ему все-равно какая часть речи должна получиться.\n",
    "\n",
    "Леммер вызывается функцией `lemmatize(word,pos)`, ему мы указываем и слово и желаемую часть речи. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-11T07:35:08.609894Z",
     "start_time": "2020-10-11T07:35:05.683727Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\neuron\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmer: seek\n",
      "Lemmatizer: seek\n",
      "\n",
      "Stemmer: drove\n",
      "Lemmatizer: drive\n",
      "\n",
      "Stemmer: meet\n",
      "Lemmatizer: meeting\n",
      "\n",
      "Stemmer: meet\n",
      "Lemmatizer: meet\n",
      "\n"
     ]
    }
   ],
   "source": [
    "morning = ((session_start_hour >= 7 ) & (session_start_hour<=11)).astype('int')\n",
    "day = ((session_start_hour >= 12 ) & (session_start_hour<= 18)).astype('int')\n",
    "evening = ((session_start_hour >= 19 ) & (session_start_hour<=23)).astype('int')\n",
    "night = ((session_start_hour >= 0 ) & (session_start_hour<=6)).astype('int')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Стоп-слова"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стоп-слова – это слова, которые выкидываются из текста до/после обработки текста. Когда мы применяем машинное обучение к текстам, такие слова могут добавить много шума, поэтому необходимо избавляться от ненужных слов.\n",
    "\n",
    "Стоп-слова это обычно артикли, междометия, союзы и т.д., которые не несут смысловой нагрузки. При этом надо понимать, что не существует универсального списка стоп-слов, все зависит от конкретного случая.\n",
    "\n",
    "В NLTK есть предустановленный список стоп-слов. Перед первым использованием вам понадобится его скачать: `nltk.download(\"stopwords\")`. После скачивания можно подключить модуль `stopwords` и посмотреть на сами слова:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-11T07:35:08.649896Z",
     "start_time": "2020-10-11T07:35:08.612894Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\neuron\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим, как можно убрать стоп-слова из предложения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-11T07:35:08.655897Z",
     "start_time": "2020-10-11T07:35:08.651896Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Backgammon', 'one', 'oldest', 'known', 'board', 'games', '.']\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words(\"english\")) # получим стоп-слова, превратим их в множество с помощью set()\n",
    "sentence = \"Backgammon is one of the oldest known board games.\" # зададим строку\n",
    "\n",
    "words = nltk.word_tokenize(sentence) # токенизируем ее по словам\n",
    "\n",
    "# и будем в цикле перебирать все слова из words, проверять входит ли оно в множество стоп-слов stop_words,\n",
    "# и если нет, то вернем слово word, иначе ничего не вернем. \n",
    "without_stop_words = [word for word in words if not word in stop_words] \n",
    "print(without_stop_words) #\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Чистка текста"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возможно, вы заметили, что, когда мы разбили текст на предложения, они выглядели неприглядно: начинались с символа пробела, содержали пустую строку в конце списка. \n",
    "\n",
    "При работе с текстами такая ситуация - не редкость. Более того, в текстах может содержаться гораздо более противный и мешающий мусор. Зачастую тексты надо чистить от персональных данных и другой чувствительной информации.\n",
    "\n",
    "Иногда бывает, что в тексты закрадываются целые блоки кода или мусора после парсинга веб страниц.\n",
    "Иногда же надо просто избавиться от определенного списка слов.\n",
    "\n",
    "Все это понимается под чисткой текста. Мы уже встречались с ее частным случаем ранее - когда удаляли из текста стоп слова методами библиотеки `nltk`. \n",
    "\n",
    "В общем случае чистка текста - нетривиальная и творческая задача: иногда достаточно сложно задать правило, благодаря которому будут найдены нужные слова. Иногда для этого даже нужно будет воспользоваться обученными NER моделями. Это все придет с практикой и в будущем.\n",
    "\n",
    "Но есть один метод, который будет применяться чаще всего и почти всегда, и это метод `replace()`, который позволяет заменить часть строки (подстроку) на другую строку."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Этот метод применяется к строке, которую хотим изменить. В качестве первого аргумента надо записать часть строки, которую хотим заменить, а в качестве второго - на что хотим.\n",
    "\n",
    "При этом обратите внимание, что сам метод не осуществляет изменения строки, если мы не выполняли операцию присваивания."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-11T07:35:08.689898Z",
     "start_time": "2020-10-11T07:35:08.656897Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Большой и интересный текст это. Привет, мир, - говорит программа. Что-то оптимизировалось.'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s # наш текст"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-11T07:35:08.722900Z",
     "start_time": "2020-10-11T07:35:08.690899Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Большой и интересный текст . Привет, мир, - говорит программа. Что-то оптимизировалось.'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.replace('это', '') # заменяем все подстроки 'это'  на пробел."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-11T07:35:08.753902Z",
     "start_time": "2020-10-11T07:35:08.724900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Большой и интересный текст это. Привет, мир, - говорит программа. Что-то оптимизировалось.'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_time['morning'] = morning\n",
    "train_time['day'] = day\n",
    "train_time['evening'] = evening\n",
    "train_time['night'] = night"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-11T07:35:08.786904Z",
     "start_time": "2020-10-11T07:35:08.754902Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Большой и интересный текст . Привет, мир, - говорит программа. Что-то оптимизировалось.'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = s.replace('это', '') # если хотим поменять - надо переприсвоить ей значение.\n",
    "s # Теперь строка изменилась"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Дополнительное чтение \n",
    " \n",
    "Дополнительно почитайте самостоятельно про __регулярные выражения__\n",
    " https://habr.com/ru/post/349860/\n",
    " \n",
    "и про парсер для русского языка `Natasha`. С его помощью можно вытаскивать некоторые именованные сущности на русском языке из текста для дальнейшего анализа и обработки, такие как адреса, даты, имена. \n",
    "https://habr.com/ru/post/349864/\n",
    "\n",
    "Посмотрите на полезный список инструментов для обработки естественных языков\n",
    "\n",
    "Там указано, для каких задач они предназначены и для каких языков. \n",
    "https://nlpub.mipt.ru/Обработка_текста\n",
    "\n",
    "Сейчас мы не будем про это говорить, но, когда будем обрабатывать тексты - еще вернемся.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Домашнее задание"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Написать на английском языке сочинение на 1 страницу о том, как вы провели лето и определить все именованные сущности и их тип в этом тексте.\n",
    "\n",
    "В тексте должно быть указано, где вы были, когда и с кем. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
